{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "LoRA was chosen to enable parameter-efficient fine-tuning of the distilbert-base-uncased model by introducing low-rank trainable adapters into specific layers. This approach reduces computational overhead by keeping the majority of the model's parameters frozen while training only a small number of additional parameters, making it ideal for resource-constrained environments. \n",
    "* Model: \n",
    "distilbert-base-uncased:\n",
    "A lightweight version of BERT, distilbert-base-uncased retains 97% of BERT's performance while being 40% smaller and faster.\n",
    "* Evaluation approach: Metrics:\n",
    "Accuracy: Measures the overall correctness of predictions.\n",
    "Precision, Recall, and F1 Score: Particularly relevant for imbalanced datasets like SMS spam detection, ensuring the model balances the trade-off between false positives and false negatives.\n",
    "* Fine-tuning dataset: \n",
    "SMS_Spam Dataset:\n",
    "A labeled dataset containing SMS messages categorized as \"spam\" or \"ham\" (not spam).\n",
    "Reason for Choice: The dataset is well-suited for binary sequence classification tasks and offers a realistic application scenario for testing the fine-tuning of a lightweight model like DistilBERT with a PEFT technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required version of datasets and prerequsites\n",
    "# Restart kernel after installing\n",
    "!pip install -q \"datasets==2.15.0\"\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm\n",
    "!pip install --upgrade datasets\n",
    "!pip install scikit-learn\n",
    "!pip install evaluate\n",
    "!pip install --upgrade evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27a66715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Load dataset (use a smaller subset for testing)\n",
    "dataset = load_dataset('sms_spam', split='train[:10%]')  # Using only 1% of the training data for testing\n",
    "\n",
    "# Model checkpoint (use distilbert-base-uncased for lighter model)\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f5ad611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# Add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    text = examples[\"sms\"]\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c4ab4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447403</td>\n",
       "      <td>0.856373</td>\n",
       "      <td>0.790116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-70 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.4707819257463728, metrics={'train_runtime': 10.758, 'train_samples_per_second': 51.775, 'train_steps_per_second': 6.507, 'total_flos': 7613511992676.0, 'train_loss': 0.4707819257463728, 'epoch': 1.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the necessary imports and dataset preprocessing have been done\n",
    "\n",
    "# Evaluation function as defined earlier\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)  # Get predicted labels\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    # Compute F1 score with 'weighted' averaging\n",
    "    f1_result = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    \n",
    "    # Return both accuracy and F1 score\n",
    "    return {\"accuracy\": accuracy_result[\"accuracy\"], \"f1\": f1_result[\"f1\"]}\n",
    "\n",
    "# Training parameters\n",
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define the Trainer for the base model (before fine-tuning)\n",
    "trainer_base = Trainer(\n",
    "    model=model,  # base model, not fine-tuned\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training the base model\n",
    "trainer_base.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b8e5dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model saved to ./base_model\n"
     ]
    }
   ],
   "source": [
    "# Save the base model\n",
    "base_model_dir = \"./base_model\"  # Directory to save the base model\n",
    "model.save_pretrained(base_model_dir)\n",
    "tokenizer.save_pretrained(base_model_dir)  # Save tokenizer as well\n",
    "print(f\"base model saved to {base_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5775fadf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.100764</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.978342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-base-uncased-lora-text-classification/checkpoint-70 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=0.11117047582353864, metrics={'train_runtime': 3.2896, 'train_samples_per_second': 169.323, 'train_steps_per_second': 21.279, 'total_flos': 7724568431304.0, 'train_loss': 0.11117047582353864, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define LORA config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=['q_lin']\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "lr = 1e-3\n",
    "batch_size = 8  # Reduced batch size for better performance on limited resources\n",
    "num_epochs = 1  # Reduced number of epochs for faster testing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer setup with the modified PEFT model\n",
    "trainer_peft = Trainer(\n",
    "    model=peft_model,  # Use the LORA-modified model\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_peft.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model saved to ./peft_model\n"
     ]
    }
   ],
   "source": [
    "# Save the PEFT model\n",
    "peft_model_dir = \"./peft_model\"  # Directory to save the PEFT model\n",
    "model.save_pretrained(peft_model_dir)\n",
    "tokenizer.save_pretrained(peft_model_dir)  # Save tokenizer as well\n",
    "print(f\"PEFT model saved to {peft_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc3a8147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Evaluation Results:\n",
      "-------------------------------\n",
      "eval_loss                : 0.4474\n",
      "eval_accuracy            : 0.8564\n",
      "eval_f1                  : 0.7901\n",
      "eval_runtime             : 1.1261\n",
      "eval_samples_per_second  : 494.6290\n",
      "eval_steps_per_second    : 62.1620\n",
      "epoch                    : 1.0000\n",
      "\n",
      "Fine-Tuned PEFT Model Evaluation Results:\n",
      "----------------------------------------\n",
      "eval_loss                : 0.1008\n",
      "eval_accuracy            : 0.9785\n",
      "eval_f1                  : 0.9783\n",
      "eval_runtime             : 1.0599\n",
      "eval_samples_per_second  : 525.5050\n",
      "eval_steps_per_second    : 66.0420\n",
      "epoch                    : 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the models from disk\n",
    "Base_model1_uploaded = AutoModelForSequenceClassification.from_pretrained(base_model_dir)\n",
    "LoRA_model1_uploaded = AutoModelForSequenceClassification.from_pretrained(peft_model_dir)\n",
    "\n",
    "# Set model to base and evaluate\n",
    "trainer.model = Base_model1_uploaded\n",
    "evaluation_base_results = trainer_base.evaluate()\n",
    "\n",
    "# Set model to PEFT and evaluate\n",
    "trainer.model = LoRA_model1_uploaded\n",
    "evaluation_peft_results = trainer_peft.evaluate()\n",
    "\n",
    "# Print results in a structured format\n",
    "print(\"\\nBase Model Evaluation Results:\")\n",
    "print(\"-------------------------------\")\n",
    "for metric, value in evaluation_base_results.items():\n",
    "    print(f\"{metric:<25}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFine-Tuned PEFT Model Evaluation Results:\")\n",
    "print(\"----------------------------------------\")\n",
    "for metric, value in evaluation_peft_results.items():\n",
    "    print(f\"{metric:<25}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550395c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
